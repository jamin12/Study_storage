# 로지스틱 회귀
## 로지스틱 회귀

1. 로지스틱 회귀 분석을 사용하는 이유
   -  종속 변수가 Y가 성공 실패인 문제에 대해 예측 모델링을 한다고 가정하자. </br> 이를 Linear Regression으로 모델링하고잫 한다면 범위가 맞치 않는 문제가 발생한다.
   -  ![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F99C6283D5E7D4C282B)
   -  Y의 범위는 0~1이여야 하는데, 연속형 변수를 모델링 하는 Linear Regression의 범위는 -무한 ~ 무한이다.
   - ![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FrogbT%2FbtqDpgxkUlr%2F0WWgbB2dbe0p6HltxDCplK%2Fimg.png)
   - 참과,거짓으로 점을 찍으면 직선으로 값을 그려내기가 어렵다.</br> s자 형태로 그래프가 그려지는 함수가 시그모이드 함수이다.
   - 시그모이드 함수 방적식
     - ![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FckDKR8%2FbtqDoUadVQM%2FJm9MAosKa95kThU5Q7I0q0%2Fimg.png)
   - z 값을 그대로 쓰게 된다면 일반적인 회귀가 되지만 시그모이드 함수를 활용해 s자로 만듦
     - ![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FCuwUl%2Fbtq8KYtwxag%2F25ZOma79MHgmFykAxrSwVk%2Fimg.png)
### 2진 분류
``` py

df = pd.read_csv("./5주차/wine_dataset.csv")

wine_feature = np.column_stack((df["volatile_acidity"],df["citric_acid"]))
wine_style = df["style"].to_numpy()

train_input, test_input, train_target, test_target = train_test_split(
    wine_feature, wine_style ,random_state=42)

lg = LogisticRegression()
lg.fit(train_input,train_target)

# 결과의 확률값을 도출 = 시그모이드 함수에 통과시킨 값과 같음
print(lr.predict_proba(train_input[:5]))

# z 를구하는 식 
decisions = lg.decision_function(test_input[:5])
print(decisions)

# 시그모이드 함수에 통과 시켜 결과값 도출 
print(expit(decisions))
```

### 다중 분류
``` py
from modulefinder import LOAD_CONST
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from scipy.special import expit
from sklearn.preprocessing import StandardScaler
from scipy.special import softmax

df = pd.read_csv("./5주차/wine_dataset.csv")

wine_feature = np.column_stack((df["volatile_acidity"],df["citric_acid"]))
wine_style = df["style"].to_numpy()

train_input, test_input, train_target, test_target = train_test_split(
    wine_feature, wine_style ,random_state=42)

# 표준화 z 스코어와 같음
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)

# C : 규제 강도(default값인 l2사용) ,max_iter : 계산에 사용할 작업 수
lg = LogisticRegression(C=20,max_iter=1000)
lg.fit(train_scaled,train_target)

print(lg.score(train_scaled,train_target))
print(lg.score(test_scaled,test_target))

print(lg.predict(test_scaled[:5]))
proba = lg.predict_proba(train_input[:5])

print(np.round(proba,decimals=3))

# z 값 얻어 내기 분류 종류에 따라 생김 분류가 7개 특성이5개 이면  (7,5) (7) 이렇게 나옴 
decision = lg.decision_function(test_scaled[:5])

# softmax를 이용해 학률 내기
proba = softmax(decision)
print(np.round(proba,decimals=3))
```